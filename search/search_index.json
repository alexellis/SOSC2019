{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"FaaS hands on session Contacts Diego Ciangottini (diego.ciangottini<at>pg.infn.it) Mirco Tracolli (mirco.tracolli<at>pg.infn.it) Original matieral Part of the FaaS introductory material is provided by OpenFaaS workshop Hands-on guide PaaS Orchestration Functions 101 Workflows and triggers EXTRAS: machine setup and architecture Ansible deployment of a webserver Container Orchestration with Kubernetes Setting up FaaS platform","title":"Home"},{"location":"#faas-hands-on-session","text":"","title":"FaaS hands on session"},{"location":"#contacts","text":"Diego Ciangottini (diego.ciangottini<at>pg.infn.it) Mirco Tracolli (mirco.tracolli<at>pg.infn.it)","title":"Contacts"},{"location":"#original-matieral","text":"Part of the FaaS introductory material is provided by OpenFaaS workshop","title":"Original matieral"},{"location":"#hands-on-guide","text":"PaaS Orchestration Functions 101 Workflows and triggers","title":"Hands-on guide"},{"location":"#extras-machine-setup-and-architecture","text":"Ansible deployment of a webserver Container Orchestration with Kubernetes Setting up FaaS platform","title":"EXTRAS: machine setup and architecture"},{"location":"ansible/","text":"\u25c0 Before starting download the hands-on repository and move inside the directory as follow: git clone https://github.com/DODAS-TS/SOSC-2018.git cd SOSC-2018 Automation with Ansible What's Ansible Ansible is a software that consistently automatize the configuration management. The Ansible configurations are a set of minimal infrastracture descriptions that are easy to read and to mantain, reducing the amount of work needed to setup environment and softwares on data centers or even on a laptop. Ansible does not require any remote agents and delivers all modules to remote systems that execute tasks, as needed, to deploy the desired configuration. Ansible Galaxy also has over 4,000 community-provided roles that can be used by anyone and tailored to different environments. In this hands-on we are going to deploy a simple web server application as a first introduction to Ansible recipes. Hello word: install and deploy a webserver with Ansible Playbooks Playbooks are the Ansible building blocks. They describe the desired infrastracture with a sequence of states and checks, that will be automatically deployed at installation time. Ansible infact implements a \"state-driven\" paradigm, that does not indicate the exact chain of commands but instead check a sequential list of machine state . But let's do an exercise to understand better the basics. install the apache2 packages on localhost --- - hosts : localhost connection : local tasks : - name : Apache | Make sure the Apache packages are installed apt : name=apache2 update_cache=yes when : ansible_os_family == \"Debian\" - name : Apache | Make sure the Apache packages are installed yum : name=httpd when : ansible_os_family == \"RedHat\" start services after configuration customization --- - hosts : localhost connection : local tasks : - name : Configure apache 1/2 get_url : force : true url : https://raw.githubusercontent.com/DODAS-TS/SOSC-2018/master/templates/hands-on-1/apache-config/port.conf dest : /etc/apache2/ports.conf - name : Configure apache 2/2 get_url : force : true url : https://raw.githubusercontent.com/DODAS-TS/SOSC-2018/master/templates/hands-on-1/apache-config/000-default.conf dest : /etc/apache2/sites-enabled/000-default.conf - name : Start Apache service service : name=apache2 state=restarted when : ansible_os_family == \"Debian\" - name : Start Apache service service : name=httpd state=restarted when : ansible_os_family == \"RedHat\" Run the playbooks install the apache2 packages on localhost ansible-playbook templates/hands-on-1/ansible-role-install.yml start services after configuration customization ansible-playbook templates/hands-on-1/ansible-role-apache.yaml the apache default webpage should now being served at localhost:4880 N.B. with this simple configuration files we will be able to reproduce with a single command the current state on different machines. Of course this is a very simple software installation that is meant as demonstrator, but in principle can be much more complicated.","title":"Ansible deployment of a webserver"},{"location":"ansible/#_1","text":"Before starting download the hands-on repository and move inside the directory as follow: git clone https://github.com/DODAS-TS/SOSC-2018.git cd SOSC-2018","title":"\u25c0"},{"location":"ansible/#automation-with-ansible","text":"","title":"Automation with Ansible"},{"location":"ansible/#whats-ansible","text":"Ansible is a software that consistently automatize the configuration management. The Ansible configurations are a set of minimal infrastracture descriptions that are easy to read and to mantain, reducing the amount of work needed to setup environment and softwares on data centers or even on a laptop. Ansible does not require any remote agents and delivers all modules to remote systems that execute tasks, as needed, to deploy the desired configuration. Ansible Galaxy also has over 4,000 community-provided roles that can be used by anyone and tailored to different environments. In this hands-on we are going to deploy a simple web server application as a first introduction to Ansible recipes.","title":"What's Ansible"},{"location":"ansible/#hello-word-install-and-deploy-a-webserver-with-ansible","text":"","title":"Hello word: install and deploy a webserver with Ansible"},{"location":"ansible/#playbooks","text":"Playbooks are the Ansible building blocks. They describe the desired infrastracture with a sequence of states and checks, that will be automatically deployed at installation time. Ansible infact implements a \"state-driven\" paradigm, that does not indicate the exact chain of commands but instead check a sequential list of machine state . But let's do an exercise to understand better the basics. install the apache2 packages on localhost --- - hosts : localhost connection : local tasks : - name : Apache | Make sure the Apache packages are installed apt : name=apache2 update_cache=yes when : ansible_os_family == \"Debian\" - name : Apache | Make sure the Apache packages are installed yum : name=httpd when : ansible_os_family == \"RedHat\" start services after configuration customization --- - hosts : localhost connection : local tasks : - name : Configure apache 1/2 get_url : force : true url : https://raw.githubusercontent.com/DODAS-TS/SOSC-2018/master/templates/hands-on-1/apache-config/port.conf dest : /etc/apache2/ports.conf - name : Configure apache 2/2 get_url : force : true url : https://raw.githubusercontent.com/DODAS-TS/SOSC-2018/master/templates/hands-on-1/apache-config/000-default.conf dest : /etc/apache2/sites-enabled/000-default.conf - name : Start Apache service service : name=apache2 state=restarted when : ansible_os_family == \"Debian\" - name : Start Apache service service : name=httpd state=restarted when : ansible_os_family == \"RedHat\"","title":"Playbooks"},{"location":"ansible/#run-the-playbooks","text":"install the apache2 packages on localhost ansible-playbook templates/hands-on-1/ansible-role-install.yml start services after configuration customization ansible-playbook templates/hands-on-1/ansible-role-apache.yaml the apache default webpage should now being served at localhost:4880 N.B. with this simple configuration files we will be able to reproduce with a single command the current state on different machines. Of course this is a very simple software installation that is meant as demonstrator, but in principle can be much more complicated.","title":"Run the playbooks"},{"location":"events/","text":"\u25c0 Working with functions You can get a local environment ready using Vagrant for an automatically setting up a machine on Virtualbox. To download Vagrant follow this link where you can find the complete list based on Os System: https://www.vagrantup.com/downloads.html Vagrant installation - Windows Download the following file and install it: https://releases.hashicorp.com/vagrant/2.2.5/vagrant_2.2.5_x86_64.msi Vagrant installation - Linux Download the package and extract it just pasting the following commands: wget https://releases.hashicorp.com/vagrant/2.2.5/vagrant_2.2.5_linux_amd64.zip unzip vagrant_2.2.5_linux_amd64.zip You can also use Homebrew like this: brew cask install vagrant If you prefer, you can use your favorite package manager. For example, in Ubuntu you can type: sudo apt install virtualbox vagrant Vagrant installation - MacOS Download the dmg file and install it: https://releases.hashicorp.com/vagrant/2.2.5/vagrant_2.2.5_x86_64.dmg If you have Homebrew installed just type this in the command line: brew cask install vagrant Install SOSC2019 Vagrant project To be operative with the current project you have to download with git or extract the zip file . Open your command line and paste the following commands: git clone https://github.com/Cloud-PG/SOSC2019.git cd SOSC2019 Now set up the vagrant environment using the following command: ./vagrant up # Or if you have vagrant executable in your PATH vagrant up NOTE : This may take few minutes, depending on network and your computer performance And then, log into the created machine: ./vagrant ssh # Or if you have vagrant executable in your PATH vagrant ssh NOTE : Vangrant and Virtualbox are required on the machine of course. If you don't have them check the previous steps to install the packages Some additional machines have been prepared for the school participants. You can find hot to access to your machine here Using example functions and WebUI Now you can go to http://localhost:31112/ui/ and, using the password in gateway_password.txt with user admin , you should be able to log in. To see your password just type: cat gateway-password.txt You will have a page like that as result after the login: Let's start playing with some example functions. For instance, you can instantiate a function the face-detection of an online image just clicking on Deploy new function , searching for opencv and installing face-detect with OpenCV (button Deploy ). Now a new tab should appear with the function name selected. From there you can check the status and also try to invoke the function from the UI. For instance, as soon as the status of the function is ready, let's try to put an url with a jpg image in the request body field and then press invoke. Let's try the two below for example: https://parismatch.be/app/uploads/2018/04/Macaca_nigra_self-portrait_large-e1524567086123-1100x715.jpg http://thedreamwithinpictures.com/wp-content/uploads/2013/05/c3a89__montage_2048-copy.jpg For the second one you will have the following result: The list of all available functions in the store is also available from CLI using the following command from the vagrant machine already created: faas-cli store list Deployment of a python function (from OpenFaaS workshop ) Do everyone have a docker account? mkdir astronaut-finder cd astronaut-finder faas-cli new --lang python3 astronaut-finder --prefix = \"<your-docker-username-here>\" Function fundamentals The previous command will write three files for us: ./astronaut-finder/handler.py This is the handler for the function. NOTE : an handler get a request object with the raw request and can print the result of the function to the console. ./astronaut-finder/requirements.txt Use this file to list any pip modules you want to install so, to manage your Python requirements, such as requests or urllib ./astronaut-finder.yml This file is used to manage the function: NOTE : it has the name of the function, the Docker image and any other customizations needed. Edit ./astronaut-finder/requirements.txt and add the following dependency: requests Write the function's code We'll be pulling in data from http://api.open-notify.org/astros.json Here's an example of the result we will have from that url: { \"number\" : 6 , \"people\" : [ { \"craft\" : \"ISS\" , \"name\" : \"Alexander Misurkin\" }, { \"craft\" : \"ISS\" , \"name\" : \"Mark Vande Hei\" }, { \"craft\" : \"ISS\" , \"name\" : \"Joe Acaba\" }, { \"craft\" : \"ISS\" , \"name\" : \"Anton Shkaplerov\" }, { \"craft\" : \"ISS\" , \"name\" : \"Scott Tingle\" }, { \"craft\" : \"ISS\" , \"name\" : \"Norishige Kanai\" } ], \"message\" : \"success\" } Let's write an handler that gets for us that result. You have to edit handler.py : import requests import random def handle ( req ): r = requests . get ( \"http://api.open-notify.org/astros.json\" ) result = r . json () index = random . randint ( 0 , result [ \"number\" ] - 1 ) name = result [ \"people\" ][ index ][ \"name\" ] return \" %s is in space\" % ( name ) Deploy the function First, build it: faas-cli build -f ./astronaut-finder.yml Push the function: docker login faas-cli push -f ./astronaut-finder.yml Deploy the function: export OPENFAAS_URL = http://127.0.0.1:31112 cat $HOME /gateway-password.txt | faas-cli login --password-stdin faas-cli deploy -f ./astronaut-finder.yml And now, just wait a bit for the function to be in Ready state. Check from cli with the following command: faas-cli describe astronaut-finder | grep Status You will have a result like this if everythings is up and running: Status: Ready and then try to invoke it from command line: echo | faas-cli invoke astronaut-finder You will receive this as response a random name of a docker user, like this: Anton Shkaplerov is in space You can also use the http endpoint: curl http://localhost:31112/function/astronaut-finder Or try it from the dashboard, just clicking to invoke and see the result in response body : HOMEWORK Try to create a function for serving your ML model (you can also make use of: https://github.com/alexellis/tensorflow-serving-openfaas ) Create a function in a different language if you know any","title":"Functions 101"},{"location":"events/#_1","text":"","title":"\u25c0"},{"location":"events/#working-with-functions","text":"You can get a local environment ready using Vagrant for an automatically setting up a machine on Virtualbox. To download Vagrant follow this link where you can find the complete list based on Os System: https://www.vagrantup.com/downloads.html","title":"Working with functions"},{"location":"events/#vagrant-installation-windows","text":"Download the following file and install it: https://releases.hashicorp.com/vagrant/2.2.5/vagrant_2.2.5_x86_64.msi","title":"Vagrant installation - Windows"},{"location":"events/#vagrant-installation-linux","text":"Download the package and extract it just pasting the following commands: wget https://releases.hashicorp.com/vagrant/2.2.5/vagrant_2.2.5_linux_amd64.zip unzip vagrant_2.2.5_linux_amd64.zip You can also use Homebrew like this: brew cask install vagrant If you prefer, you can use your favorite package manager. For example, in Ubuntu you can type: sudo apt install virtualbox vagrant","title":"Vagrant installation - Linux"},{"location":"events/#vagrant-installation-macos","text":"Download the dmg file and install it: https://releases.hashicorp.com/vagrant/2.2.5/vagrant_2.2.5_x86_64.dmg If you have Homebrew installed just type this in the command line: brew cask install vagrant","title":"Vagrant installation - MacOS"},{"location":"events/#install-sosc2019-vagrant-project","text":"To be operative with the current project you have to download with git or extract the zip file . Open your command line and paste the following commands: git clone https://github.com/Cloud-PG/SOSC2019.git cd SOSC2019 Now set up the vagrant environment using the following command: ./vagrant up # Or if you have vagrant executable in your PATH vagrant up NOTE : This may take few minutes, depending on network and your computer performance And then, log into the created machine: ./vagrant ssh # Or if you have vagrant executable in your PATH vagrant ssh NOTE : Vangrant and Virtualbox are required on the machine of course. If you don't have them check the previous steps to install the packages Some additional machines have been prepared for the school participants. You can find hot to access to your machine here","title":"Install SOSC2019 Vagrant project"},{"location":"events/#using-example-functions-and-webui","text":"Now you can go to http://localhost:31112/ui/ and, using the password in gateway_password.txt with user admin , you should be able to log in. To see your password just type: cat gateway-password.txt You will have a page like that as result after the login: Let's start playing with some example functions. For instance, you can instantiate a function the face-detection of an online image just clicking on Deploy new function , searching for opencv and installing face-detect with OpenCV (button Deploy ). Now a new tab should appear with the function name selected. From there you can check the status and also try to invoke the function from the UI. For instance, as soon as the status of the function is ready, let's try to put an url with a jpg image in the request body field and then press invoke. Let's try the two below for example: https://parismatch.be/app/uploads/2018/04/Macaca_nigra_self-portrait_large-e1524567086123-1100x715.jpg http://thedreamwithinpictures.com/wp-content/uploads/2013/05/c3a89__montage_2048-copy.jpg For the second one you will have the following result: The list of all available functions in the store is also available from CLI using the following command from the vagrant machine already created: faas-cli store list","title":"Using example functions and WebUI"},{"location":"events/#deployment-of-a-python-function-from-openfaas-workshop","text":"Do everyone have a docker account? mkdir astronaut-finder cd astronaut-finder faas-cli new --lang python3 astronaut-finder --prefix = \"<your-docker-username-here>\"","title":"Deployment of a python function (from OpenFaaS workshop)"},{"location":"events/#function-fundamentals","text":"The previous command will write three files for us: ./astronaut-finder/handler.py This is the handler for the function. NOTE : an handler get a request object with the raw request and can print the result of the function to the console. ./astronaut-finder/requirements.txt Use this file to list any pip modules you want to install so, to manage your Python requirements, such as requests or urllib ./astronaut-finder.yml This file is used to manage the function: NOTE : it has the name of the function, the Docker image and any other customizations needed. Edit ./astronaut-finder/requirements.txt and add the following dependency: requests","title":"Function fundamentals"},{"location":"events/#write-the-functions-code","text":"We'll be pulling in data from http://api.open-notify.org/astros.json Here's an example of the result we will have from that url: { \"number\" : 6 , \"people\" : [ { \"craft\" : \"ISS\" , \"name\" : \"Alexander Misurkin\" }, { \"craft\" : \"ISS\" , \"name\" : \"Mark Vande Hei\" }, { \"craft\" : \"ISS\" , \"name\" : \"Joe Acaba\" }, { \"craft\" : \"ISS\" , \"name\" : \"Anton Shkaplerov\" }, { \"craft\" : \"ISS\" , \"name\" : \"Scott Tingle\" }, { \"craft\" : \"ISS\" , \"name\" : \"Norishige Kanai\" } ], \"message\" : \"success\" } Let's write an handler that gets for us that result. You have to edit handler.py : import requests import random def handle ( req ): r = requests . get ( \"http://api.open-notify.org/astros.json\" ) result = r . json () index = random . randint ( 0 , result [ \"number\" ] - 1 ) name = result [ \"people\" ][ index ][ \"name\" ] return \" %s is in space\" % ( name )","title":"Write the function's code"},{"location":"events/#deploy-the-function","text":"First, build it: faas-cli build -f ./astronaut-finder.yml Push the function: docker login faas-cli push -f ./astronaut-finder.yml Deploy the function: export OPENFAAS_URL = http://127.0.0.1:31112 cat $HOME /gateway-password.txt | faas-cli login --password-stdin faas-cli deploy -f ./astronaut-finder.yml And now, just wait a bit for the function to be in Ready state. Check from cli with the following command: faas-cli describe astronaut-finder | grep Status You will have a result like this if everythings is up and running: Status: Ready and then try to invoke it from command line: echo | faas-cli invoke astronaut-finder You will receive this as response a random name of a docker user, like this: Anton Shkaplerov is in space You can also use the http endpoint: curl http://localhost:31112/function/astronaut-finder Or try it from the dashboard, just clicking to invoke and see the result in response body :","title":"Deploy the function"},{"location":"events/#homework","text":"Try to create a function for serving your ML model (you can also make use of: https://github.com/alexellis/tensorflow-serving-openfaas ) Create a function in a different language if you know any","title":"HOMEWORK"},{"location":"k3s/","text":"\u25c0 Orchestrating containers From Kuberntes overview - \"Containers are a good way to bundle and run your applications. In a production environment, you need to manage the containers that run the applications and ensure that there is no downtime. For example, if a container goes down, another container needs to start. Wouldn\u2019t it be easier if this behavior was handled by a system?\" Let's now put our hands on how Kubernetes can help in managing containers. Setup of a local instance In this hands-on we will use K3s for a quick start on your machine but there are plenty of similar solution that you can choose, later below you can find references for some of them. To install it, just execute: curl -sfL https://get.k3s.io | sh - And the verify that the cluster is up and running with: sudo kubectl get nodes Kubernetes fundamentals From Kubernetes docs - \"Kubernetes Objects are persistent entities in the Kubernetes system. Kubernetes uses these entities to represent the state of your cluster. Specifically, they can describe: What containerized applications are running (and on which nodes) The resources available to those applications The policies around how those applications behave, such as restart policies, upgrades, and fault-tolerance A Kubernetes object is a \u201crecord of intent\u201d\u2013once you create the object, the Kubernetes system will constantly work to ensure that object exists. By creating an object, you\u2019re effectively telling the Kubernetes system what you want your cluster\u2019s workload to look like; this is your cluster\u2019s desired state. To work with Kubernetes objects\u2013whether to create, modify, or delete them\u2013you\u2019ll need to use the Kubernetes API. When you use the kubectl command-line interface, for example, the CLI makes the necessary Kubernetes API calls for you. You can also use the Kubernetes API directly in your own programs using one of the Client Libraries.\" In this hands-on we will quickly walk through two main objects: Pods and Services. We will use, with some adaptations, Kubernetes by example material, where you can find many other examples for different objects and use cases. Homeworks Other references https://kubernetes.io/docs/tasks/tools/install-minikube/ https://github.com/kubernetes-sigs/kind https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/ http://kubernetesbyexample.com/","title":"Setup a Container Orchestration"},{"location":"k3s/#_1","text":"","title":"\u25c0"},{"location":"k3s/#orchestrating-containers","text":"From Kuberntes overview - \"Containers are a good way to bundle and run your applications. In a production environment, you need to manage the containers that run the applications and ensure that there is no downtime. For example, if a container goes down, another container needs to start. Wouldn\u2019t it be easier if this behavior was handled by a system?\" Let's now put our hands on how Kubernetes can help in managing containers.","title":"Orchestrating containers"},{"location":"k3s/#setup-of-a-local-instance","text":"In this hands-on we will use K3s for a quick start on your machine but there are plenty of similar solution that you can choose, later below you can find references for some of them. To install it, just execute: curl -sfL https://get.k3s.io | sh - And the verify that the cluster is up and running with: sudo kubectl get nodes","title":"Setup of a local instance"},{"location":"k3s/#kubernetes-fundamentals","text":"From Kubernetes docs - \"Kubernetes Objects are persistent entities in the Kubernetes system. Kubernetes uses these entities to represent the state of your cluster. Specifically, they can describe: What containerized applications are running (and on which nodes) The resources available to those applications The policies around how those applications behave, such as restart policies, upgrades, and fault-tolerance A Kubernetes object is a \u201crecord of intent\u201d\u2013once you create the object, the Kubernetes system will constantly work to ensure that object exists. By creating an object, you\u2019re effectively telling the Kubernetes system what you want your cluster\u2019s workload to look like; this is your cluster\u2019s desired state. To work with Kubernetes objects\u2013whether to create, modify, or delete them\u2013you\u2019ll need to use the Kubernetes API. When you use the kubectl command-line interface, for example, the CLI makes the necessary Kubernetes API calls for you. You can also use the Kubernetes API directly in your own programs using one of the Client Libraries.\" In this hands-on we will quickly walk through two main objects: Pods and Services. We will use, with some adaptations, Kubernetes by example material, where you can find many other examples for different objects and use cases.","title":"Kubernetes fundamentals"},{"location":"k3s/#homeworks","text":"","title":"Homeworks"},{"location":"k3s/#other-references","text":"https://kubernetes.io/docs/tasks/tools/install-minikube/ https://github.com/kubernetes-sigs/kind https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/ http://kubernetesbyexample.com/","title":"Other references"},{"location":"openfaas/","text":"\u25c0 Setting up a FaaS platform With function we mean a small and reusable piece of code that: is short-lived is not a daemon (long-running) is not stateful makes use of your existing services or third-party resources usually executes in a few seconds There is a reach zoology of FaaS frameworks created to provide and to manage functions starting from the one developed by IaaS providers (e.g. AWS Lambda, Google Cloud Functions, Azure functions). There are also many open-source FaaS solutions (as you can see in the figure above) and for this hands-on we will make use of OpenFaaS . Getting things ready on Kubernetes sudo kubectl apply -f https://raw.githubusercontent.com/openfaas/faas-netes/master/namespaces.yml # generate a random password PASSWORD = $( head -c 12 /dev/urandom | shasum | cut -d ' ' -f1 ) sudo kubectl -n openfaas create secret generic basic-auth \\ --from-literal = basic-auth-user = admin \\ --from-literal = basic-auth-password = \" $PASSWORD \" echo $PASSWORD > gateway-password.txt git clone https://github.com/openfaas/faas-netes cd faas-netes && \\ sudo kubectl apply -f ./yaml After few minutes you should be able to go to http://127.0.0.1:31112 to start browsing the OpenFaas WebUI. curl -sL cli.openfaas.com | sudo sh $ faas-cli help $ faas-cli version export OPENFAAS_URL = http://127.0.0.1:31112 echo -n $PASSWORD | faas-cli login --password-stdin Homeworks Other references https://kubernetes.io/docs/tasks/tools/install-minikube/ https://github.com/kubernetes-sigs/kind https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/ http://kubernetesbyexample.com/","title":"Setup FaaS platform"},{"location":"openfaas/#_1","text":"","title":"\u25c0"},{"location":"openfaas/#setting-up-a-faas-platform","text":"With function we mean a small and reusable piece of code that: is short-lived is not a daemon (long-running) is not stateful makes use of your existing services or third-party resources usually executes in a few seconds There is a reach zoology of FaaS frameworks created to provide and to manage functions starting from the one developed by IaaS providers (e.g. AWS Lambda, Google Cloud Functions, Azure functions). There are also many open-source FaaS solutions (as you can see in the figure above) and for this hands-on we will make use of OpenFaaS .","title":"Setting up a FaaS platform"},{"location":"openfaas/#getting-things-ready-on-kubernetes","text":"sudo kubectl apply -f https://raw.githubusercontent.com/openfaas/faas-netes/master/namespaces.yml # generate a random password PASSWORD = $( head -c 12 /dev/urandom | shasum | cut -d ' ' -f1 ) sudo kubectl -n openfaas create secret generic basic-auth \\ --from-literal = basic-auth-user = admin \\ --from-literal = basic-auth-password = \" $PASSWORD \" echo $PASSWORD > gateway-password.txt git clone https://github.com/openfaas/faas-netes cd faas-netes && \\ sudo kubectl apply -f ./yaml After few minutes you should be able to go to http://127.0.0.1:31112 to start browsing the OpenFaas WebUI. curl -sL cli.openfaas.com | sudo sh $ faas-cli help $ faas-cli version export OPENFAAS_URL = http://127.0.0.1:31112 echo -n $PASSWORD | faas-cli login --password-stdin","title":"Getting things ready on Kubernetes"},{"location":"openfaas/#homeworks","text":"","title":"Homeworks"},{"location":"openfaas/#other-references","text":"https://kubernetes.io/docs/tasks/tools/install-minikube/ https://github.com/kubernetes-sigs/kind https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/ http://kubernetesbyexample.com/","title":"Other references"},{"location":"orchent/","text":"\u25c0 Introduction to cloud platforms When working with cloud resources, depending on the user needs, different layers of underlyng abstraction can be needed, and depending on how many layers and their composition one can define different categories. Platform as a Service on top of Infrastracture as a Service Infrastructure as a service (IaaS) is a cloud computing offering in which a vendor provides users access to computing resources such as servers , storage and networking. Organizations use their own platforms and applications within a service provider\u2019s infrastructure. Key features: Instead of purchasing hardware outright, users pay for IaaS on demand. Infrastructure is scalable depending on processing and storage needs. Saves enterprises the costs of buying and maintaining their own hardware. Because data is on the cloud, there can be no single point of failure. Enables the virtualization of administrative tasks, freeing up time for other work. Platform as a service (PaaS) is a cloud computing offering that provides users with a cloud environment in which they can develop, manage and deliver applications. In addition to storage and other computing resources, users are able to use a suite of prebuilt tools to develop, customize and test their own applications. Key features: SaaS vendors provide users with software and applications via a subscription model. Users do not have to manage, install or upgrade software; SaaS providers manage this. Data is secure in the cloud; equipment failure does not result in loss of data. Use of resources can be scaled depending on service needs. Applications are accessible from almost any internet-connected device, from virtually anywhere in the world. N.B. In this hands-on a simple VM will be deployed, as an example, on cloud resources in an automated way thanks the use of a PaaS orchestrator and TOSCA system description files. More complicated recipices can provide you with a working k8s cluster where you can setup a FaaS framework as you will use in the next chapters. INDIGO-DC PaaS orchestrator The INDIGO PaaS Orchestrator allows to instantiate resources on Cloud Management Frameworks (like OpenStack and OpenNebula) platforms based on deployment requests that are expressed through templates written in TOSCA YAML Simple Profile v1.0 , and deploys them on the best cloud site available. Requirement First of you need to register to the service as described here . N.B. please put in the registration note \"SOSC2019 student\". Requests without this note will not be accepted. Please also notice that the resources instantiated for the school will be removed from the test pool few days after the end of the school. Install deployment client sudo apt install -y jq unzip wget https://github.com/Cloud-PG/dodas-go-client/releases/download/v0.2.2/dodas.zip unzip dodas.zip sudo mv dodas /usr/local/bin/ Retrieve IAM token git clone https://github.com/Cloud-PG/SOSC2019.git cd SOSC2019 source ./scripts/get_orchent_token.sh You'll be prompted with username and password requests. Just insert the one corresponding to you Indigo-IAM account. Using TOSCA The TOSCA metamodel uses the concept of service templates to describe cloud workloads as a topology template , which is a graph of node templates modeling the components a workload is made up of and as relationship templates modeling the relations between those components. TOSCA further provides a type system of node types to describe the possible building blocks for constructing a service template , as well as relationship type to describe possible kinds of relations. Both node and relationship types may define lifecycle operations to implement the behavior an orchestration engine can invoke when instantiating a service template. For example, a node type for some software product might provide a \u2018create\u2019 operation to handle the creation of an instance of a component at runtime, or a \u2018start\u2019 or \u2018stop\u2019 operation to handle a start or stop event triggered by an orchestration engine. Those lifecycle operations are backed by implementation artifacts such as scripts or Chef recipes that implement the actual behavior. The TOSCA simple profile assumes a number of base types (node types and relationship types) to be supported by each compliant environment such as a \u2018Compute\u2019 node type, a \u2018Network\u2019 node type or a generic \u2018Database\u2019 node type. Furthermore, it is envisioned that a large number of additional types for use in service templates will be defined by a community over time . Therefore, template authors in many cases will not have to define types themselves but can simply start writing service templates that use existing types . In addition, the simple profile will provide means for easily customizing and extending existing types, for example by providing a customized \u2018create\u2019 script for some software. Deploy a simple VM on the cloud: TOSCA types Tosca types are the building blocks needed to indicate the correct procedure for the vm creation and software deployment. During this hands on the following types are used: tosca_definitions_version : tosca_simple_yaml_1_0 capability_types : tosca.capabilities.indigo.OperatingSystem : derived_from : tosca.capabilities.OperatingSystem properties : gpu_driver : type : boolean required : no cuda_support : type : boolean required : no cuda_min_version : type : string required : no image : type : string required : no credential : type : tosca.datatypes.Credential required : no tosca.capabilities.indigo.Scalable : derived_from : tosca.capabilities.Scalable properties : min_instances : type : integer default : 1 required : no max_instances : type : integer default : 1 required : no count : type : integer description : the number of resources required : no default : 1 removal_list : type : list description : list of IDs of the resources to be removed required : no entry_schema : type : string tosca.capabilities.indigo.Container : derived_from : tosca.capabilities.Container properties : instance_type : type : string required : no num_gpus : type : integer required : false gpu_vendor : type : string required : false gpu_model : type : string required : false tosca.capabilities.indigo.Endpoint : derived_from : tosca.capabilities.Endpoint properties : dns_name : description : The optional name to register with DNS type : string required : false private_ip : description : Flag used to specify that this endpoint will require also a private IP although it is a public one. type : boolean required : false default : true attributes : credential : type : list entry_schema : type : tosca.datatypes.Credential artifact_types : tosca.artifacts.Implementation.YAML : derived_from : tosca.artifacts.Implementation description : YAML Ansible recipe artifact mime_type : text/yaml file_ext : [ yaml , yml ] tosca.artifacts.AnsibleGalaxy.role : derived_from : tosca.artifacts.Root description : Ansible Galaxy role to be deployed in the target node relationship_types : tosca.relationships.indigo.Manages : derived_from : tosca.relationships.Root node_types : tosca.nodes.WebServer.Apache : derived_from : tosca.nodes.WebServer interfaces : Standard : create : implementation : https://raw.githubusercontent.com/DODAS-TS/SOSC-2019/master/templates/hands-on-1/ansible-role-install.yml start : implementation : https://raw.githubusercontent.com/DODAS-TS/SOSC-2019/master/templates/hands-on-1/ansible-role-apache.yml tosca.nodes.indigo.Compute : derived_from : tosca.nodes.indigo.MonitoredCompute attributes : private_address : type : list entry_schema : type : string public_address : type : list entry_schema : type : string ctxt_log : type : string capabilities : scalable : type : tosca.capabilities.indigo.Scalable os : type : tosca.capabilities.indigo.OperatingSystem endpoint : type : tosca.capabilities.indigo.Endpoint host : type : tosca.capabilities.indigo.Container valid_source_types : [ tosca.nodes.SoftwareComponent ] tosca.nodes.indigo.MonitoredCompute : derived_from : tosca.nodes.Compute properties : # Set the current data of the zabbix server # but it can also specified in the TOSCA document zabbix_server : type : string required : no default : orchestrator.cloud.cnaf.infn.it zabbix_server_port : type : tosca.datatypes.network.PortDef required : no default : 10051 zabbix_server_metadata : type : string required : no default : Linux 668c875e-9a39-4dc0-a710-17c41376c1e0 artifacts : zabbix_agent_role : file : indigo-dc.zabbix-agent type : tosca.artifacts.AnsibleGalaxy.role interfaces : Standard : configure : implementation : https://raw.githubusercontent.com/indigo-dc/tosca-types/master/artifacts/zabbix/zabbix_agent_install.yml inputs : zabbix_server : { get_property : [ SELF , zabbix_server ] } zabbix_server_port : { get_property : [ SELF , zabbix_server_port ] } zabbix_server_metadata : { get_property : [ SELF , zabbix_server_metadata ] } Deploy command with deployment template The deployment template makes use of the TOSCA types defined above to create and orchestrate the deployment on the cloud resources. tosca_definitions_version : tosca_simple_yaml_1_0 imports : - indigo_custom_types : https://raw.githubusercontent.com/DODAS-TS/SOSC-2019/master/templates/common/types.yml description : TOSCA template for a complete CMS Site over Mesos orchestrator topology_template : inputs : input test : type : string default : \"test\" node_templates : create-server-vm : type : tosca.nodes.indigo.Compute capabilities : endpoint : properties : network_name : PUBLIC dns_name : serverpublic scalable : properties : count : 1 host : properties : num_cpus : 1 mem_size : \"2 GB\" os : properties : image : \"ost://cloud.recas.ba.infn.it/1113d7e8-fc5d-43b9-8d26-61906d89d479\" outputs : vm_ip : value : { concat : [ get_attribute : [ create-server-vm , public_address , 0 ] ] } cluster_credentials : value : { get_attribute : [ create-server-vm , endpoint , credential , 0 ] } Before starting with the deployment let's validate our template: dodas validate --template templates/hands-on-1/Handson-Part1.yaml Then start the deployment on provided cloud resources with: dodas create --config auth_file.yaml templates/hands-on-1/Handson-Part1.yaml the expected output is something like: Using config file: auth_file.yaml validate called Template OK Template: templates/hands-on-1/Handson-Part1.yaml Submitting request to : https://im-dodas.cloud.cnaf.infn.it/infrastructures InfrastructureID: 69a25fce-d947-11e9-b18c-0242ac120003 Note down you InfrastructureID. Monitor the deployment process Check the status of the deployment time to time with: dodas --config auth_file.yaml get status <InfrastructureID> When completed just check how many VMs are available $ dodas --config auth_file.yaml list vms <InfrastructureID> Using config file: auth_file.yaml vms called Submitting request to : https://im-dodas.cloud.cnaf.infn.it/infrastructures Available Infrastructure VMs: https://im-dodas.cloud.cnaf.infn.it/infrastructures/69a25fce-d947-11e9-b18c-0242ac120003/vms/0 Now you can retrieve the vm0 details and save the private key in a file (e.g. vm0-key) : dodas --config auth_file.yaml get vm <InfrastructureID> 0 # now write down the private key provided into vm0-key file then chmod 600 vm0-key Now you should be able to log into the machine with: ssh -i vm0-key <public address provided above> HOMEWORKS Create an automatic deployment of a webserver with Ansible Just to get an idea, try to take a look at the TOSCA file for the deployment of kubernetes cluster that uses INDIGO-DC PaaS Orchestrator and Ansible recipes here","title":"PaaS Orchestration"},{"location":"orchent/#_1","text":"","title":"\u25c0"},{"location":"orchent/#introduction-to-cloud-platforms","text":"When working with cloud resources, depending on the user needs, different layers of underlyng abstraction can be needed, and depending on how many layers and their composition one can define different categories.","title":"Introduction to cloud platforms"},{"location":"orchent/#platform-as-a-service-on-top-of-infrastracture-as-a-service","text":"Infrastructure as a service (IaaS) is a cloud computing offering in which a vendor provides users access to computing resources such as servers , storage and networking. Organizations use their own platforms and applications within a service provider\u2019s infrastructure. Key features: Instead of purchasing hardware outright, users pay for IaaS on demand. Infrastructure is scalable depending on processing and storage needs. Saves enterprises the costs of buying and maintaining their own hardware. Because data is on the cloud, there can be no single point of failure. Enables the virtualization of administrative tasks, freeing up time for other work. Platform as a service (PaaS) is a cloud computing offering that provides users with a cloud environment in which they can develop, manage and deliver applications. In addition to storage and other computing resources, users are able to use a suite of prebuilt tools to develop, customize and test their own applications. Key features: SaaS vendors provide users with software and applications via a subscription model. Users do not have to manage, install or upgrade software; SaaS providers manage this. Data is secure in the cloud; equipment failure does not result in loss of data. Use of resources can be scaled depending on service needs. Applications are accessible from almost any internet-connected device, from virtually anywhere in the world. N.B. In this hands-on a simple VM will be deployed, as an example, on cloud resources in an automated way thanks the use of a PaaS orchestrator and TOSCA system description files. More complicated recipices can provide you with a working k8s cluster where you can setup a FaaS framework as you will use in the next chapters.","title":"Platform as a Service on top of Infrastracture as a Service"},{"location":"orchent/#indigo-dc-paas-orchestrator","text":"The INDIGO PaaS Orchestrator allows to instantiate resources on Cloud Management Frameworks (like OpenStack and OpenNebula) platforms based on deployment requests that are expressed through templates written in TOSCA YAML Simple Profile v1.0 , and deploys them on the best cloud site available.","title":"INDIGO-DC PaaS orchestrator"},{"location":"orchent/#requirement","text":"First of you need to register to the service as described here . N.B. please put in the registration note \"SOSC2019 student\". Requests without this note will not be accepted. Please also notice that the resources instantiated for the school will be removed from the test pool few days after the end of the school.","title":"Requirement"},{"location":"orchent/#install-deployment-client","text":"sudo apt install -y jq unzip wget https://github.com/Cloud-PG/dodas-go-client/releases/download/v0.2.2/dodas.zip unzip dodas.zip sudo mv dodas /usr/local/bin/","title":"Install deployment client"},{"location":"orchent/#retrieve-iam-token","text":"git clone https://github.com/Cloud-PG/SOSC2019.git cd SOSC2019 source ./scripts/get_orchent_token.sh You'll be prompted with username and password requests. Just insert the one corresponding to you Indigo-IAM account.","title":"Retrieve IAM token"},{"location":"orchent/#using-tosca","text":"The TOSCA metamodel uses the concept of service templates to describe cloud workloads as a topology template , which is a graph of node templates modeling the components a workload is made up of and as relationship templates modeling the relations between those components. TOSCA further provides a type system of node types to describe the possible building blocks for constructing a service template , as well as relationship type to describe possible kinds of relations. Both node and relationship types may define lifecycle operations to implement the behavior an orchestration engine can invoke when instantiating a service template. For example, a node type for some software product might provide a \u2018create\u2019 operation to handle the creation of an instance of a component at runtime, or a \u2018start\u2019 or \u2018stop\u2019 operation to handle a start or stop event triggered by an orchestration engine. Those lifecycle operations are backed by implementation artifacts such as scripts or Chef recipes that implement the actual behavior. The TOSCA simple profile assumes a number of base types (node types and relationship types) to be supported by each compliant environment such as a \u2018Compute\u2019 node type, a \u2018Network\u2019 node type or a generic \u2018Database\u2019 node type. Furthermore, it is envisioned that a large number of additional types for use in service templates will be defined by a community over time . Therefore, template authors in many cases will not have to define types themselves but can simply start writing service templates that use existing types . In addition, the simple profile will provide means for easily customizing and extending existing types, for example by providing a customized \u2018create\u2019 script for some software.","title":"Using TOSCA"},{"location":"orchent/#deploy-a-simple-vm-on-the-cloud-tosca-types","text":"Tosca types are the building blocks needed to indicate the correct procedure for the vm creation and software deployment. During this hands on the following types are used: tosca_definitions_version : tosca_simple_yaml_1_0 capability_types : tosca.capabilities.indigo.OperatingSystem : derived_from : tosca.capabilities.OperatingSystem properties : gpu_driver : type : boolean required : no cuda_support : type : boolean required : no cuda_min_version : type : string required : no image : type : string required : no credential : type : tosca.datatypes.Credential required : no tosca.capabilities.indigo.Scalable : derived_from : tosca.capabilities.Scalable properties : min_instances : type : integer default : 1 required : no max_instances : type : integer default : 1 required : no count : type : integer description : the number of resources required : no default : 1 removal_list : type : list description : list of IDs of the resources to be removed required : no entry_schema : type : string tosca.capabilities.indigo.Container : derived_from : tosca.capabilities.Container properties : instance_type : type : string required : no num_gpus : type : integer required : false gpu_vendor : type : string required : false gpu_model : type : string required : false tosca.capabilities.indigo.Endpoint : derived_from : tosca.capabilities.Endpoint properties : dns_name : description : The optional name to register with DNS type : string required : false private_ip : description : Flag used to specify that this endpoint will require also a private IP although it is a public one. type : boolean required : false default : true attributes : credential : type : list entry_schema : type : tosca.datatypes.Credential artifact_types : tosca.artifacts.Implementation.YAML : derived_from : tosca.artifacts.Implementation description : YAML Ansible recipe artifact mime_type : text/yaml file_ext : [ yaml , yml ] tosca.artifacts.AnsibleGalaxy.role : derived_from : tosca.artifacts.Root description : Ansible Galaxy role to be deployed in the target node relationship_types : tosca.relationships.indigo.Manages : derived_from : tosca.relationships.Root node_types : tosca.nodes.WebServer.Apache : derived_from : tosca.nodes.WebServer interfaces : Standard : create : implementation : https://raw.githubusercontent.com/DODAS-TS/SOSC-2019/master/templates/hands-on-1/ansible-role-install.yml start : implementation : https://raw.githubusercontent.com/DODAS-TS/SOSC-2019/master/templates/hands-on-1/ansible-role-apache.yml tosca.nodes.indigo.Compute : derived_from : tosca.nodes.indigo.MonitoredCompute attributes : private_address : type : list entry_schema : type : string public_address : type : list entry_schema : type : string ctxt_log : type : string capabilities : scalable : type : tosca.capabilities.indigo.Scalable os : type : tosca.capabilities.indigo.OperatingSystem endpoint : type : tosca.capabilities.indigo.Endpoint host : type : tosca.capabilities.indigo.Container valid_source_types : [ tosca.nodes.SoftwareComponent ] tosca.nodes.indigo.MonitoredCompute : derived_from : tosca.nodes.Compute properties : # Set the current data of the zabbix server # but it can also specified in the TOSCA document zabbix_server : type : string required : no default : orchestrator.cloud.cnaf.infn.it zabbix_server_port : type : tosca.datatypes.network.PortDef required : no default : 10051 zabbix_server_metadata : type : string required : no default : Linux 668c875e-9a39-4dc0-a710-17c41376c1e0 artifacts : zabbix_agent_role : file : indigo-dc.zabbix-agent type : tosca.artifacts.AnsibleGalaxy.role interfaces : Standard : configure : implementation : https://raw.githubusercontent.com/indigo-dc/tosca-types/master/artifacts/zabbix/zabbix_agent_install.yml inputs : zabbix_server : { get_property : [ SELF , zabbix_server ] } zabbix_server_port : { get_property : [ SELF , zabbix_server_port ] } zabbix_server_metadata : { get_property : [ SELF , zabbix_server_metadata ] }","title":"Deploy a simple VM on the cloud: TOSCA types"},{"location":"orchent/#deploy-command-with-deployment-template","text":"The deployment template makes use of the TOSCA types defined above to create and orchestrate the deployment on the cloud resources. tosca_definitions_version : tosca_simple_yaml_1_0 imports : - indigo_custom_types : https://raw.githubusercontent.com/DODAS-TS/SOSC-2019/master/templates/common/types.yml description : TOSCA template for a complete CMS Site over Mesos orchestrator topology_template : inputs : input test : type : string default : \"test\" node_templates : create-server-vm : type : tosca.nodes.indigo.Compute capabilities : endpoint : properties : network_name : PUBLIC dns_name : serverpublic scalable : properties : count : 1 host : properties : num_cpus : 1 mem_size : \"2 GB\" os : properties : image : \"ost://cloud.recas.ba.infn.it/1113d7e8-fc5d-43b9-8d26-61906d89d479\" outputs : vm_ip : value : { concat : [ get_attribute : [ create-server-vm , public_address , 0 ] ] } cluster_credentials : value : { get_attribute : [ create-server-vm , endpoint , credential , 0 ] } Before starting with the deployment let's validate our template: dodas validate --template templates/hands-on-1/Handson-Part1.yaml Then start the deployment on provided cloud resources with: dodas create --config auth_file.yaml templates/hands-on-1/Handson-Part1.yaml the expected output is something like: Using config file: auth_file.yaml validate called Template OK Template: templates/hands-on-1/Handson-Part1.yaml Submitting request to : https://im-dodas.cloud.cnaf.infn.it/infrastructures InfrastructureID: 69a25fce-d947-11e9-b18c-0242ac120003 Note down you InfrastructureID.","title":"Deploy command with deployment template"},{"location":"orchent/#monitor-the-deployment-process","text":"Check the status of the deployment time to time with: dodas --config auth_file.yaml get status <InfrastructureID> When completed just check how many VMs are available $ dodas --config auth_file.yaml list vms <InfrastructureID> Using config file: auth_file.yaml vms called Submitting request to : https://im-dodas.cloud.cnaf.infn.it/infrastructures Available Infrastructure VMs: https://im-dodas.cloud.cnaf.infn.it/infrastructures/69a25fce-d947-11e9-b18c-0242ac120003/vms/0 Now you can retrieve the vm0 details and save the private key in a file (e.g. vm0-key) : dodas --config auth_file.yaml get vm <InfrastructureID> 0 # now write down the private key provided into vm0-key file then chmod 600 vm0-key Now you should be able to log into the machine with: ssh -i vm0-key <public address provided above>","title":"Monitor the deployment process"},{"location":"orchent/#homeworks","text":"Create an automatic deployment of a webserver with Ansible Just to get an idea, try to take a look at the TOSCA file for the deployment of kubernetes cluster that uses INDIGO-DC PaaS Orchestrator and Ansible recipes here","title":"HOMEWORKS"},{"location":"workflows/","text":"\u25c0 Workflows - a.k.a. functions calling functions Building your first workflow - from OpenFaaS workshop Using the CLI to deploy SentimentAnalysis function from the store: mkdir $HOME /workflows cd $HOME /workflows export OPENFAAS_URL = http://127.0.0.1:31112 faas-cli store deploy SentimentAnalysis The Sentiment Analysis function will tell you the subjectivity and polarity (positivity rating) of any sentence. The result of the function is formatted in JSON as you can see with the example below: echo -n \"California is great, it's always sunny there.\" | faas-cli invoke sentimentanalysis # Formatted result { \"polarity\" : 0.8 , \"sentence_count\" : 1 , \"subjectivity\" : 0.75 } Now let's create a new simple function (like in the previous exercise) that will call sentimentanalysis just forwarding the request text. faas-cli new --lang python3 invoker --prefix = \"<your-docker-username-here>\" The handler.py code should look like this: import os import requests import sys def handle ( req ): \"\"\"handle a request to the function Args: req (str): request body \"\"\" gateway_hostname = os . getenv ( \"gateway_hostname\" , \"gateway\" ) test_sentence = req r = requests . get ( \"http://\" + gateway_hostname + \":8080/function/sentimentanalysis\" , data = test_sentence ) if r . status_code != 200 : sys . exit ( \"Error with sentimentanalysis, expected: %d , got: %d \\n \" % ( 200 , r . status_code )) result = r . json () if result [ \"polarity\" ] > 0.45 : return \"That was probably positive\" else : return \"That was neutral or negative\" Put requests in requirements.txt file: echo \"requests\" >> invoker/requirements.txt Remember to set the environment variable gateway_hostname in invoker.yml : version : 1.0 provider : name : openfaas gateway : http://127.0.0.1:31112 functions : invoker : lang : python3 handler : ./invoker image : mircot/invoker:latest environment : gateway_hostname : \"gateway.openfaas\" Then, just deploy our function: faas-cli up -f invoker.yml You can now try to invoke the new function. You can verify that the request has been forwarded to sentimentanalysis by your custom function. We have just created a basic workflow. $ echo -n \"California is bad, it's always rainy there.\" | faas-cli invoke invoker That was neutral or negative $ echo -n \"California is great, it's always sunny there.\" | faas-cli invoke invoker That was probably positive Triggers Example: using storage events webhook If you are using Vagrant image you can start from here, otherwise at the end you'll find how to setup a S3 object storage on your own. Let's configure it properly for using a webhook that points to our openfaas instance (we will use it later to trigger a function as soon as a new file appears). mc admin config set local < /home/vagrant/config_minio.json mc admin service restart local The request sent to the function by Minio in case of a file upload will have a body in this form: { \"EventName\" : \"s3:ObjectCreated:Put\" , \"Key\" : \"images/test7.jpg\" , \"Records\" : [ { \"eventVersion\" : \"2.0\" , \"eventSource\" : \"minio:s3\" , \"awsRegion\" : \"\" , \"eventTime\" : \"2019-09-10T14:27:46Z\" , \"eventName\" : \"s3:ObjectCreated:Put\" , \"userIdentity\" : { \"principalId\" : \"usernameID\" }, \"requestParameters\" : { \"accessKey\" : \"myaccesskey\" , \"region\" : \"\" , \"sourceIPAddress\" : \"192.168.0.213\" }, \"responseElements\" : { \"content-length\" : \"0\" , \"x-amz-request-id\" : \"15C319FC231726B5\" , \"x-minio-deployment-id\" : \"f6a78fdc-8d8e-4d2c-8aca-4b0bd4082129\" , \"x-minio-origin-endpoint\" : \"http://192.168.0.213:9000\" }, \"s3\" : { \"s3SchemaVersion\" : \"1.0\" , \"configurationId\" : \"Config\" , \"bucket\" : { \"name\" : \"images\" , \"ownerIdentity\" : { \"principalId\" : \"usernameID\" }, \"arn\" : \"arn:aws:s3:::images\" }, \"object\" : { \"key\" : \"test7.jpg\" , \"size\" : 1767621 , \"eTag\" : \"1f9ae70259a36b5c1b5692f91386bb75-1\" , \"contentType\" : \"image/jpeg\" , \"userMetadata\" : { \"content-type\" : \"image/jpeg\" }, \"versionId\" : \"1\" , \"sequencer\" : \"15C319FC2679B7CB\" } }, \"source\" : { \"host\" : \"192.168.0.213\" , \"port\" : \"\" , \"userAgent\" : \"MinIO (linux; amd64) minio-go/v6.0.32 mc/2019-09-05T23:43:50Z\" } } ] } Now create two buckets called incoming and processed : mc mb local/incoming mc mb local/processed Set the trigger for any new jpg file appearing in local/incoming: mc event add local/incoming arn:minio:sqs::1:webhook --event put --suffix .jpg You can log into the WebUI at http://localhost:9000/ui/ with username admin and password adminminio . From there you can upload files and check the contents of the buckets. Trigger a facedetect function on loaded images First of all create a new function: mkdir $HOME /triggers cd $HOME /triggers faas-cli new --lang python3 processimage --prefix = \"<your-docker-username-here>\" Then we need to modify the handler to: get the file name from the storage event get the file from the storage encode it in base64 (required as input by the face detection function) call the face detection function get the output and save it back to the storage in a separate bucket A possible result could be: import json from minio import Minio import requests import os import base64 def handle ( st ): \"\"\"handle a request to the function Args: st (str): request body \"\"\" # Decode the json from the Minio event req = json . loads ( st ) # Get configuration parameters from the docker environment (set in the processimage.yml) gateway = os . getenv ( \"openfaas_gw\" , \"gateway.openfaas\" ) # Configure the storage client mc = Minio ( os . environ [ 'minio_hostname' ], access_key = os . environ [ 'minio_access_key' ], secret_key = os . environ [ 'minio_secret_key' ], secure = False ) # Set the name for the source and destination buckets source_bucket = \"incoming\" dest_bucket = \"processed\" # Get the name of the file from the 'Key' field in the event message file_name = req [ 'Key' ] . split ( '/' )[ - 1 ] # Get the file from the storage mc . fget_object ( source_bucket , file_name , \"/tmp/\" + file_name ) # Encode the image into base64 f = open ( \"/tmp/\" + file_name , \"rb\" ) input_image = base64 . b64encode ( f . read ()) # Pass it to the facedetect function r = requests . post ( gateway + \"/function/facedetect\" , input_image ) if r . status_code != 200 : return \"Error during call to facedetect, expected: %d , got: %d \\n \" % ( 200 , r . status_code ) # Finally get the output and save it locally dest_file_name = f \"processed_{file_name}\" f = open ( \"/tmp/\" + dest_file_name , \"wb\" ) f . write ( r . content ) f . close () f = open ( \"/tmp/input_\" + file_name , \"wb\" ) f . write ( input_image ) f . close () # sync to Minio mc . fput_object ( dest_bucket , dest_file_name , \"/tmp/\" + dest_file_name ) return f \"Image {file_name} processed. Result is in {dest_bucket}\" Now you need to configure the deployment of the functions: version : 1.0 provider : name : openfaas gateway : http://127.0.0.1:31112 functions : # function for loading the image from storage - the code just edited processimage : lang : python3 handler : ./processimage image : <your-docker-username-here>/processimage:latest environment : write_debug : true # environment variables used inside the funcion code minio_hostname : \"10.42.0.1:9000\" minio_access_key : \"admin\" minio_secret_key : \"adminminio\" openfaas_gw : \"http://gateway.openfaas:8080\" # face detection function, pre-built. You can find the source here: # https://github.com/alexellis/facedetect-openfaas facedetect : skip_build : true image : alexellis2/facedetect:0.1 environment : output_mode : \"image\" write_debug : true Before pushing the function in, don't forget to set the requirements.txt: minio requests Then just build and deploy our two functions with: faas-cli build -f processimage.yml faas-cli push -f processimage.yml faas-cli deploy -f processimage.yml Now, once the functions will be ready you should try to upload a .jpg image to the incoming bucket using the WebUI ( login at <your host>:9000 with user admin and passwd adminminio ) and soon you should be able to find a processed file in the processed bucket that you can download from the webUI and visualize. In the following image you can see an example of the hook result: HOMEWORKS Create a workflow with 2 functions in different languages Try to create a workflow triggered by a storage event that use the Tensorflow serving function created on the previous set of homeworks EXTRA: Setting up an S3-compatible storage mkdir $HOME /minio_data docker run -d -v $HOME /minio_data:/data --net host -e \"MINIO_ACCESS_KEY=admin\" -e \"MINIO_SECRET_KEY=admindciangot\" minio/minio server /data and the client wget https://dl.min.io/client/mc/release/linux-amd64/mc mv mc /usr/bin/mc sudo chmod +x /usr/bin/mc","title":"Workflows and triggers"},{"location":"workflows/#_1","text":"","title":"\u25c0"},{"location":"workflows/#workflows-aka-functions-calling-functions","text":"","title":"Workflows - a.k.a. functions calling functions"},{"location":"workflows/#building-your-first-workflow-from-openfaas-workshop","text":"Using the CLI to deploy SentimentAnalysis function from the store: mkdir $HOME /workflows cd $HOME /workflows export OPENFAAS_URL = http://127.0.0.1:31112 faas-cli store deploy SentimentAnalysis The Sentiment Analysis function will tell you the subjectivity and polarity (positivity rating) of any sentence. The result of the function is formatted in JSON as you can see with the example below: echo -n \"California is great, it's always sunny there.\" | faas-cli invoke sentimentanalysis # Formatted result { \"polarity\" : 0.8 , \"sentence_count\" : 1 , \"subjectivity\" : 0.75 } Now let's create a new simple function (like in the previous exercise) that will call sentimentanalysis just forwarding the request text. faas-cli new --lang python3 invoker --prefix = \"<your-docker-username-here>\" The handler.py code should look like this: import os import requests import sys def handle ( req ): \"\"\"handle a request to the function Args: req (str): request body \"\"\" gateway_hostname = os . getenv ( \"gateway_hostname\" , \"gateway\" ) test_sentence = req r = requests . get ( \"http://\" + gateway_hostname + \":8080/function/sentimentanalysis\" , data = test_sentence ) if r . status_code != 200 : sys . exit ( \"Error with sentimentanalysis, expected: %d , got: %d \\n \" % ( 200 , r . status_code )) result = r . json () if result [ \"polarity\" ] > 0.45 : return \"That was probably positive\" else : return \"That was neutral or negative\" Put requests in requirements.txt file: echo \"requests\" >> invoker/requirements.txt Remember to set the environment variable gateway_hostname in invoker.yml : version : 1.0 provider : name : openfaas gateway : http://127.0.0.1:31112 functions : invoker : lang : python3 handler : ./invoker image : mircot/invoker:latest environment : gateway_hostname : \"gateway.openfaas\" Then, just deploy our function: faas-cli up -f invoker.yml You can now try to invoke the new function. You can verify that the request has been forwarded to sentimentanalysis by your custom function. We have just created a basic workflow. $ echo -n \"California is bad, it's always rainy there.\" | faas-cli invoke invoker That was neutral or negative $ echo -n \"California is great, it's always sunny there.\" | faas-cli invoke invoker That was probably positive","title":"Building your first workflow - from OpenFaaS workshop"},{"location":"workflows/#triggers","text":"","title":"Triggers"},{"location":"workflows/#example-using-storage-events-webhook","text":"If you are using Vagrant image you can start from here, otherwise at the end you'll find how to setup a S3 object storage on your own. Let's configure it properly for using a webhook that points to our openfaas instance (we will use it later to trigger a function as soon as a new file appears). mc admin config set local < /home/vagrant/config_minio.json mc admin service restart local The request sent to the function by Minio in case of a file upload will have a body in this form: { \"EventName\" : \"s3:ObjectCreated:Put\" , \"Key\" : \"images/test7.jpg\" , \"Records\" : [ { \"eventVersion\" : \"2.0\" , \"eventSource\" : \"minio:s3\" , \"awsRegion\" : \"\" , \"eventTime\" : \"2019-09-10T14:27:46Z\" , \"eventName\" : \"s3:ObjectCreated:Put\" , \"userIdentity\" : { \"principalId\" : \"usernameID\" }, \"requestParameters\" : { \"accessKey\" : \"myaccesskey\" , \"region\" : \"\" , \"sourceIPAddress\" : \"192.168.0.213\" }, \"responseElements\" : { \"content-length\" : \"0\" , \"x-amz-request-id\" : \"15C319FC231726B5\" , \"x-minio-deployment-id\" : \"f6a78fdc-8d8e-4d2c-8aca-4b0bd4082129\" , \"x-minio-origin-endpoint\" : \"http://192.168.0.213:9000\" }, \"s3\" : { \"s3SchemaVersion\" : \"1.0\" , \"configurationId\" : \"Config\" , \"bucket\" : { \"name\" : \"images\" , \"ownerIdentity\" : { \"principalId\" : \"usernameID\" }, \"arn\" : \"arn:aws:s3:::images\" }, \"object\" : { \"key\" : \"test7.jpg\" , \"size\" : 1767621 , \"eTag\" : \"1f9ae70259a36b5c1b5692f91386bb75-1\" , \"contentType\" : \"image/jpeg\" , \"userMetadata\" : { \"content-type\" : \"image/jpeg\" }, \"versionId\" : \"1\" , \"sequencer\" : \"15C319FC2679B7CB\" } }, \"source\" : { \"host\" : \"192.168.0.213\" , \"port\" : \"\" , \"userAgent\" : \"MinIO (linux; amd64) minio-go/v6.0.32 mc/2019-09-05T23:43:50Z\" } } ] } Now create two buckets called incoming and processed : mc mb local/incoming mc mb local/processed Set the trigger for any new jpg file appearing in local/incoming: mc event add local/incoming arn:minio:sqs::1:webhook --event put --suffix .jpg You can log into the WebUI at http://localhost:9000/ui/ with username admin and password adminminio . From there you can upload files and check the contents of the buckets.","title":"Example: using storage events webhook"},{"location":"workflows/#trigger-a-facedetect-function-on-loaded-images","text":"First of all create a new function: mkdir $HOME /triggers cd $HOME /triggers faas-cli new --lang python3 processimage --prefix = \"<your-docker-username-here>\" Then we need to modify the handler to: get the file name from the storage event get the file from the storage encode it in base64 (required as input by the face detection function) call the face detection function get the output and save it back to the storage in a separate bucket A possible result could be: import json from minio import Minio import requests import os import base64 def handle ( st ): \"\"\"handle a request to the function Args: st (str): request body \"\"\" # Decode the json from the Minio event req = json . loads ( st ) # Get configuration parameters from the docker environment (set in the processimage.yml) gateway = os . getenv ( \"openfaas_gw\" , \"gateway.openfaas\" ) # Configure the storage client mc = Minio ( os . environ [ 'minio_hostname' ], access_key = os . environ [ 'minio_access_key' ], secret_key = os . environ [ 'minio_secret_key' ], secure = False ) # Set the name for the source and destination buckets source_bucket = \"incoming\" dest_bucket = \"processed\" # Get the name of the file from the 'Key' field in the event message file_name = req [ 'Key' ] . split ( '/' )[ - 1 ] # Get the file from the storage mc . fget_object ( source_bucket , file_name , \"/tmp/\" + file_name ) # Encode the image into base64 f = open ( \"/tmp/\" + file_name , \"rb\" ) input_image = base64 . b64encode ( f . read ()) # Pass it to the facedetect function r = requests . post ( gateway + \"/function/facedetect\" , input_image ) if r . status_code != 200 : return \"Error during call to facedetect, expected: %d , got: %d \\n \" % ( 200 , r . status_code ) # Finally get the output and save it locally dest_file_name = f \"processed_{file_name}\" f = open ( \"/tmp/\" + dest_file_name , \"wb\" ) f . write ( r . content ) f . close () f = open ( \"/tmp/input_\" + file_name , \"wb\" ) f . write ( input_image ) f . close () # sync to Minio mc . fput_object ( dest_bucket , dest_file_name , \"/tmp/\" + dest_file_name ) return f \"Image {file_name} processed. Result is in {dest_bucket}\" Now you need to configure the deployment of the functions: version : 1.0 provider : name : openfaas gateway : http://127.0.0.1:31112 functions : # function for loading the image from storage - the code just edited processimage : lang : python3 handler : ./processimage image : <your-docker-username-here>/processimage:latest environment : write_debug : true # environment variables used inside the funcion code minio_hostname : \"10.42.0.1:9000\" minio_access_key : \"admin\" minio_secret_key : \"adminminio\" openfaas_gw : \"http://gateway.openfaas:8080\" # face detection function, pre-built. You can find the source here: # https://github.com/alexellis/facedetect-openfaas facedetect : skip_build : true image : alexellis2/facedetect:0.1 environment : output_mode : \"image\" write_debug : true Before pushing the function in, don't forget to set the requirements.txt: minio requests Then just build and deploy our two functions with: faas-cli build -f processimage.yml faas-cli push -f processimage.yml faas-cli deploy -f processimage.yml Now, once the functions will be ready you should try to upload a .jpg image to the incoming bucket using the WebUI ( login at <your host>:9000 with user admin and passwd adminminio ) and soon you should be able to find a processed file in the processed bucket that you can download from the webUI and visualize. In the following image you can see an example of the hook result:","title":"Trigger a facedetect function on loaded images"},{"location":"workflows/#homeworks","text":"Create a workflow with 2 functions in different languages Try to create a workflow triggered by a storage event that use the Tensorflow serving function created on the previous set of homeworks","title":"HOMEWORKS"},{"location":"workflows/#extra-setting-up-an-s3-compatible-storage","text":"mkdir $HOME /minio_data docker run -d -v $HOME /minio_data:/data --net host -e \"MINIO_ACCESS_KEY=admin\" -e \"MINIO_SECRET_KEY=admindciangot\" minio/minio server /data and the client wget https://dl.min.io/client/mc/release/linux-amd64/mc mv mc /usr/bin/mc sudo chmod +x /usr/bin/mc","title":"EXTRA: Setting up an S3-compatible storage"}]}